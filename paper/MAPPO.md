#### MAPPO（2022，将基础的单agent算法扩展到muti-agent上，==推荐，业界标杆star高，代码多==）

文章的核心内容是探讨 **Proximal Policy Optimization (PPO)** 在协作多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）中的表现。尽管 PPO 通常在单智能体环境中表现优异，但在多智能体场景中的研究较少。

1. **PPO 在多智能体环境中的表现**
   通过实验证明，经过简单调整的 PPO 方法在多智能体协作任务中可以达到甚至超越流行的离策略算法（如 MADDPG 和 QMix），特别是在样本效率和最终性能方面表现出色。
2. **实验范围**
   研究在四种流行的多智能体基准测试环境中进行了实验，包括：
   - Multi-Agent Particle World Environments (MPE)
   - StarCraft Multi-Agent Challenge (SMAC)
   - Google Research Football (GRF)
   - Hanabi Challenge
     这些环境涵盖了从简单的物理协作到复杂的战略协作。
3. **关键贡献**
   - PPO 在无需领域特定修改或复杂调参的情况下，在协作多智能体任务中达到了优异表现。
   - 文章通过消融实验，分析了影响 PPO 表现的关键因素，例如值函数归一化、训练数据使用、剪辑参数等，并提供了实际调参建议。
   - 证明了参数共享策略在大多数场景下能够显著提高训练效率。
4. **重要发现**
   - PPO 的简单调整（如中心化值函数输入）能够显著提升其在多智能体任务中的效果。
   - 在大多数环境中，PPO 的样本效率与性能与离策略算法（如 QMix 和 RODE）相当甚至更优。
5. **建议**
   - 使用值函数归一化来稳定训练过程。
   - 在值函数输入中结合全局特征和局部特征。
   - 在较困难的环境中减少训练轮数和小批量数，以提高稳定性。